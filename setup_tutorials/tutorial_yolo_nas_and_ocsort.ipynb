{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOfePjvtWE5moyYORkee/Zm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itberrios/CV_tracking/blob/main/setup_tutorials/tutorial_yolo_nas_and_ocsort.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **YOLO-NAS + OC-SORT**\n",
        "\n",
        "This notebook contains a tutorial that shows how to incorporate YOLO-NAS with OC-SORT to perform real time visual tracking on a data stream. In this case, we will use a YouTube video to simulate a data stream of image frames.\n",
        "\n",
        "## **YOLO-NAS**\n",
        "[YOLO-NAS](https://github.com/Deci-AI/super-gradients/blob/master/YOLONAS.md) is a powerful object detector with an optimal neural network architecture that has been selected using Neural Architecture Search (NAS), hence the name NAS. \n",
        "\n",
        "At the time of release, it outperforms all of the other single shot object detectos in terms of speed and accuracy. It also excels in an area that most single shot detector struggle with, small objects. Two-stage detectors typically perform better than single stage detectors on small objects, at the cost of increased detection time [source](https://arxiv.org/pdf/1907.09408.pdf). YOLO-NAS, howver seems to provide a good tradeoff between detection speed and accuracy on small objects that previous versions of YOLO have not been able to deliver.\n",
        "\n",
        "## **OC-SORT**\n",
        "[OC-SORT](https://arxiv.org/abs/2203.14360) is a robust visual obejct tracking algorithm that improves upon the already popular [SORT](https://arxiv.org/abs/1602.00763) algorithm.\n",
        "\n",
        "SORT tends to loose track on obejcts when they are lost for extended periods of time or when non-linear motion occurs. Algorithms such as Deep SORT have effectoively improved SORT in these scenarios with a Deep Association metric that is computed with a [Siamese Neural Network](https://arxiv.org/pdf/1707.02131.pdf) over the image patches. Eventhough this is effective it comes with the cost of increased detection time due to the deep association and the Siamese network needs to be trained on in-domain data for this approach to be effective. OC-SORT on the otherhand is able to effectively increase tracking performance in a model free fashion with minimal impact to inference speed.\n",
        "\n",
        "OC-SORT introduces\n",
        "- Observation Centric Re-Update (ORU)\n",
        "    - Reduces accumulated Kalman Filter error/uncertainty when a lost track is re-associated\n",
        "- Observation Centric Momentum (OCM)\n",
        "    - Uses previous observations to compute a low noise expected motion direction and incorporates it into the track association cost\n",
        "- Observation Centric Recovery (OCR)\n",
        "    - Uses the last known observation as a secondary association to help prevent lost tracks\n",
        "\n",
        "For more details and a break down of each technique that OC-SORT introduces, please see this [article](https://medium.com/@itberrios6/introduction-to-ocsort-c1ea1c6adfa2)."
      ],
      "metadata": {
        "id": "3KMFMnBdToro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install Libraries"
      ],
      "metadata": {
        "id": "OlhbIxdPUBDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sketchy fix for \"https://stackoverflow.com/questions/73711994/importerror-cannot-import-name-is-directory-from-pil-util-usr-local-lib\"\n",
        "!pip install fastcore -U"
      ],
      "metadata": {
        "id": "1bWrvYHJcItP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# supergradients installs\n",
        "! pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113 &> /dev/null\n",
        "! pip install super-gradients\n",
        "! pip install pytorch-quantization==2.1.2 --extra-index-url https://pypi.ngc.nvidia.com &> /dev/null\n",
        "! pip install matplotlib==3.1.3 &> /dev/null\n",
        "! pip install --upgrade psutil==5.9.2 &> /dev/null\n",
        "! pip install --upgrade pillow==7.1.2 &> /dev/null"
      ],
      "metadata": {
        "id": "6I-qMzr9xeIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcOJRYPVTkeX"
      },
      "outputs": [],
      "source": [
        "!pip install filterpy\n",
        "!pip install pytube\n",
        "!pip install moviepy\n",
        "!pip install ffmpeg\n",
        "\n",
        "# bug fix for imageio-ffmpeg\n",
        "!pip install imageio==2.4.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get OCSORT code"
      ],
      "metadata": {
        "id": "YizzdBUQhJua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir ocsort\n",
        "%cd ocsort\n",
        "!wget https://raw.githubusercontent.com/noahcao/OC_SORT/master/trackers/ocsort_tracker/ocsort.py \n",
        "!wget https://raw.githubusercontent.com/noahcao/OC_SORT/master/trackers/ocsort_tracker/kalmanfilter.py \n",
        "!wget https://raw.githubusercontent.com/noahcao/OC_SORT/master/trackers/ocsort_tracker/association.py \n",
        "\n",
        "%cd .."
      ],
      "metadata": {
        "id": "vEm99f9QjUNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "lK2duJ_tl3ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import filterpy\n",
        "\n",
        "import torch\n",
        "import super_gradients as sg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 10)"
      ],
      "metadata": {
        "id": "fBNXLP7qUArx"
      },
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download video from YouTube"
      ],
      "metadata": {
        "id": "zEJO2FbwZVZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytube import YouTube\n",
        "\n",
        "# current bug in PYTube prevents this code from working\n",
        "\n",
        "url = r\"https://www.youtube.com/watch?v=JteKbauGolo\"\n",
        "yt = YouTube(url)\n",
        "print(\"Video Title: \", yt.title)\n",
        "\n",
        "# download video\n",
        "video_path = yt.streams \\\n",
        "  .filter(progressive=True, file_extension='mp4') \\\n",
        "  .order_by('resolution') \\\n",
        "  .desc() \\\n",
        "  .first() \\\n",
        "  .download() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EawjQ0FGYp0Q",
        "outputId": "8efc4d1d-c666-421d-8ad8-dc885db6284e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video Title:  Chastain BANZAI last lap move!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate YOLO-NAS model"
      ],
      "metadata": {
        "id": "O3AJIDUcZoXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from super_gradients.training import models\n",
        "from super_gradients.common.object_names import Models\n",
        "\n",
        "model = models.get(\"yolo_nas_s\", pretrained_weights=\"coco\").cuda()"
      ],
      "metadata": {
        "id": "1iQWTcJiZp4j"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantize model for better speed\n",
        "\n",
        "FOR SOME REASON THIS ACTUALLY REDUCES THE SPEED BY A FACTOR OF 10!"
      ],
      "metadata": {
        "id": "PmI6o-o1xXG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from super_gradients.training.utils.quantization.selective_quantization_utils import SelectiveQuantizer\n",
        "\n",
        "\n",
        "# cls_model = models.get(model_name=\"resnet50\", pretrained_weights=\"imagenet\")\n",
        "\n",
        "q_util = SelectiveQuantizer(\n",
        "    default_quant_modules_calibrator_weights=\"max\",\n",
        "    default_quant_modules_calibrator_inputs=\"histogram\",\n",
        "    default_per_channel_quant_weights=True,\n",
        "    default_learn_amax=False,\n",
        "    verbose=True,\n",
        ")\n",
        "q_util.quantize_module(model)"
      ],
      "metadata": {
        "id": "Mi7O-pIoxWv6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform inference on Simulated Video Stream\n",
        "\n",
        "#### First we will use MoviePy to get the frame rate and save the audio for later"
      ],
      "metadata": {
        "id": "YfcSFg7QZumm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "videoclip = VideoFileClip(video_path)\n",
        "audioclip = videoclip.audio\n",
        "\n",
        "video_fps = videoclip.fps\n",
        "video_fps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQrzaiCTZzPh",
        "outputId": "d20243cd-3d33-4519-e980-43584775f9fb"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29.97002997002997"
            ]
          },
          "metadata": {},
          "execution_count": 292
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiate tracker object"
      ],
      "metadata": {
        "id": "04FGsaIPlGXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ocsort import ocsort\n",
        "\n",
        "tracker = ocsort.OCSort(det_thresh=0.25)"
      ],
      "metadata": {
        "id": "se9puRt3lJYC"
      },
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function for bounding box colors"
      ],
      "metadata": {
        "id": "OkR9e1zBq0Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import colorsys    \n",
        "\n",
        "def get_color(number):\n",
        "    \"\"\" Converts an integer number to a color \"\"\"\n",
        "    # change these however you want to\n",
        "    hue = number*30 % 180\n",
        "    saturation = number*103 % 256\n",
        "    value = number*50 % 256\n",
        "\n",
        "    # expects normalized values\n",
        "    color = colorsys.hsv_to_rgb(hue/179, saturation/255, value/255)\n",
        "\n",
        "    return [int(c*255) for c in color]"
      ],
      "metadata": {
        "id": "ksE8TjoRqrid"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now we can simualate the data stream using opencv\n",
        "\n",
        "Make sure to reset the tracker each time you run the inference"
      ],
      "metadata": {
        "id": "1aFunGfzdqbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get frame info for tracker and video saving \n",
        "h, w = (720, 1280)\n",
        "h2, w2 = h//2, w//2\n",
        "# h2, w2 = 640, 640\n",
        "img_size = (h2, w2) \n",
        "img_info = (h2, w2)"
      ],
      "metadata": {
        "id": "wcOkR-KDl0dQ"
      },
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ensure tracker is reset\n",
        "tracker = ocsort.OCSort(det_thresh=0.35, max_age=20, delta_t=5, inertia=0.1, use_byte=True)\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "if (cap.isOpened() == False):\n",
        "    print(\"Error opening video file\")\n",
        "\n",
        "frames = []\n",
        "i = 0\n",
        "counter, fps, elapsed = 0, 0, 0\n",
        "start_time = time.perf_counter()\n",
        "\n",
        "while(cap.isOpened()):\n",
        "\n",
        "  # read each video frame\n",
        "  ret, frame = cap.read()\n",
        "\n",
        "  if ret == True:\n",
        "\n",
        "    # read image and resize by half\n",
        "    og_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    frame = cv2.resize(og_frame, \n",
        "                       (w2, h2), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # perform inference on small frame\n",
        "    predictions = model.predict(images=[frame], iou=0.25, conf=0.35)\n",
        "\n",
        "    # get detections\n",
        "    img_preds = list(predictions._images_prediction_lst)[0]\n",
        "    dets = np.hstack((img_preds.prediction.bboxes_xyxy, \n",
        "                      np.c_[img_preds.prediction.confidence]))\n",
        "\n",
        "    # update tracker\n",
        "    tracks = tracker.update(dets, img_info, img_size)\n",
        "\n",
        "    # draw tracks on frame\n",
        "    for track in tracker.trackers:\n",
        "      \n",
        "      track_id = track.id\n",
        "      hits = track.hits\n",
        "      color = get_color(track_id*15)\n",
        "      x1,y1,x2,y2 = np.round(track.get_state()).astype(int).squeeze()*2\n",
        "\n",
        "      cv2.rectangle(og_frame, (x1,y1),(x2,y2), color, 2)\n",
        "      cv2.putText(og_frame, \n",
        "                  f\"{track_id}-{hits}\", \n",
        "                  (x1+10,y1+10), \n",
        "                  cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                  0.5,\n",
        "                  color, \n",
        "                  1,\n",
        "                  cv2.LINE_AA)\n",
        "      \n",
        "    # update FPS and place on frame\n",
        "    # elapsed = (time.perf_counter() - start_time)\n",
        "    # fps = np.round((i+1)/elapsed)\n",
        "    current_time = time.perf_counter()\n",
        "    elapsed = (current_time - start_time)\n",
        "    counter += 1\n",
        "    if elapsed > 1:\n",
        "        fps = counter / elapsed;\n",
        "        counter = 0;\n",
        "        start_time = current_time;\n",
        "\n",
        "\n",
        "    cv2.putText(og_frame, \n",
        "                f\"FPS: {np.round(fps, 2)}\", \n",
        "                (10,h - 10), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                1,\n",
        "                (255,255,255), \n",
        "                2,\n",
        "                cv2.LINE_AA)\n",
        "\n",
        "    # append to list\n",
        "    frames.append(og_frame)\n",
        "\n",
        "    # # TEMP for debug\n",
        "    # if i == 10:\n",
        "    #   break\n",
        "    # else:\n",
        "    #   i += 1\n",
        "\n",
        "  # Break the loop\n",
        "  else:\n",
        "    break\n",
        "\n",
        "# When everything done, release\n",
        "# the video capture object\n",
        "cap.release()\n",
        " \n",
        "# Closes all the frames\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "q112t7wldeHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(frames[-1])"
      ],
      "metadata": {
        "id": "Y6WDBJ_vnv_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now let's put this in a video"
      ],
      "metadata": {
        "id": "ckO5TU4Us6zi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save to mp4\n",
        "out = cv2.VideoWriter('chastain_wall_ride.mp4',\n",
        "                      cv2.VideoWriter_fourcc(*'MP4V'), \n",
        "                      video_fps,\n",
        "                      (w, h))\n",
        " \n",
        "for frame in frames:\n",
        "    out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
        "    \n",
        "out.release()\n",
        "del out"
      ],
      "metadata": {
        "id": "3G2sb5YWs6gc"
      },
      "execution_count": 312,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now add back the audio"
      ],
      "metadata": {
        "id": "rnvTeROgtXZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import CompositeAudioClip\n",
        "detection_video = VideoFileClip('chastain_wall_ride.mp4')\n",
        "\n",
        "# add sound and save\n",
        "detection_video.audio = CompositeAudioClip([audioclip])\n",
        "detection_video.write_videofile('chastain_wall_ride_with_audio.mp4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5wQoICAtRzt",
        "outputId": "b7f43b96-3f97-4e34-93cf-fe19144086bf"
      },
      "execution_count": 314,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video chastain_wall_ride_with_audio.mp4.\n",
            "MoviePy - Writing audio in chastain_wall_ride_with_audioTEMP_MPY_wvf_snd.mp3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "t:   0%|          | 4/1049 [00:00<00:26, 39.63it/s, now=None]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MoviePy - Done.\n",
            "Moviepy - Writing video chastain_wall_ride_with_audio.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e0UjFFbntbqX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}